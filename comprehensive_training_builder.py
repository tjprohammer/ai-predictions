#!/usr/bin/env python3
"""
Comprehensive 120-Day Model Training Builder
==========================================
Complete training script that includes ALL sophisticated advanced features
for the 203-feature enhanced MLB prediction system.

Features:
- All 203 database features including sophisticated advanced features
- 120-day comprehensive training window (2025-03-20 to 2025-08-21)
- Advanced feature engineering with multi-game historical context
- Sophisticated sabermetric calculations
- Proper variance filtering and feature selection
- Complete training-production feature alignment

Sophisticated Advanced Features Included:
- Defensive Efficiency (810+ unique values)
- Bullpen Fatigue Score (390+ unique values) 
- Weighted Runs Scored/Allowed (435+ unique values)
- Expected Run Value Differential (356+ unique values)
- Clutch Factor (338+ unique values)
- Offensive/Early/Late Inning Efficiency
- Recent Momentum and Baseball Intelligence

Usage:
    python comprehensive_training_builder.py --comprehensive
"""

import os
import sys
import json
import joblib
import argparse
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
import psycopg2
from sqlalchemy import create_engine, text
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# Database configuration
DB_URL = "postgresql+psycopg2://mlbuser:mlbpass@localhost:5432/mlb"
MODELS_DIR = Path("models")

def fetch_comprehensive_training_data(start_date="2025-03-20", end_date="2025-08-21"):
    """
    Fetch comprehensive training data with ALL 203 features including sophisticated advanced features.
    """
    print(f"üìä COMPREHENSIVE TRAINING DATA FETCH")
    print(f"   Period: {start_date} to {end_date}")
    print(f"   Target: ALL 203 features with sophisticated calculations")
    
    conn = psycopg2.connect(
        host='localhost',
        database='mlb', 
        user='mlbuser',
        password='mlbpass'
    )
    
    # Fetch ALL features from enhanced_games
    query = """
        SELECT *
        FROM enhanced_games 
        WHERE date >= %s AND date <= %s
        AND total_runs IS NOT NULL
        AND market_total IS NOT NULL
        AND total_runs BETWEEN 5 AND 13
        AND market_total BETWEEN 5 AND 13
        ORDER BY date, game_id
    """
    
    df = pd.read_sql(query, conn, params=(start_date, end_date))
    conn.close()
    
    print(f"‚úÖ Loaded {len(df)} games with complete outcomes")
    print(f"   Features: {len(df.columns)} total columns")
    print(f"   Date range: {df['date'].min()} to {df['date'].max()}")
    
    return df

def verify_sophisticated_features(df):
    """
    Verify that our sophisticated advanced features are present and working.
    """
    print(f"\nüéØ SOPHISTICATED FEATURE VERIFICATION")
    print("-" * 50)
    
    sophisticated_features = [
        'home_team_defensive_efficiency',
        'home_team_bullpen_fatigue_score', 
        'home_team_weighted_runs_scored',
        'home_team_xrv_differential',
        'home_team_clutch_factor',
        'home_team_offensive_efficiency',
        'away_team_defensive_efficiency',
        'away_team_bullpen_fatigue_score',
        'away_team_weighted_runs_scored',
        'away_team_xrv_differential',
        'away_team_clutch_factor',
        'away_team_offensive_efficiency'
    ]
    
    missing_features = []
    quality_features = []
    
    for feature in sophisticated_features:
        if feature not in df.columns:
            missing_features.append(feature)
        else:
            unique_values = df[feature].nunique()
            null_count = df[feature].isnull().sum()
            
            if unique_values > 50:  # Good variance
                quality_features.append((feature, unique_values, null_count))
                print(f"   ‚úÖ {feature[:35]:35}: {unique_values:4} unique values, {null_count:4} nulls")
            else:
                print(f"   ‚ö†Ô∏è  {feature[:35]:35}: {unique_values:4} unique values (low variance)")
    
    if missing_features:
        print(f"\n‚ùå MISSING SOPHISTICATED FEATURES:")
        for feature in missing_features:
            print(f"   - {feature}")
        return False
    
    print(f"\n‚úÖ SOPHISTICATED FEATURES VERIFIED!")
    print(f"   {len(quality_features)} sophisticated features with excellent variance")
    print(f"   Ready for comprehensive model training!")
    
    return True

def prepare_comprehensive_features(df):
    """
    Prepare all features for training with proper handling of sophisticated features.
    """
    print(f"\nüîß COMPREHENSIVE FEATURE PREPARATION")
    print("-" * 50)
    
    # Identify feature groups
    id_columns = ['game_id', 'date', 'home_team', 'away_team']
    outcome_columns = ['total_runs', 'home_runs', 'away_runs']
    betting_columns = ['market_total', 'over_odds', 'under_odds', 'edge', 'recommendation', 'confidence']
    
    # Get all numeric features except IDs and outcomes
    numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()
    
    # Remove IDs, outcomes, and betting info from training features
    training_features = [col for col in numeric_features 
                        if col not in id_columns + outcome_columns + betting_columns]
    
    print(f"   Total numeric features: {len(numeric_features)}")
    print(f"   Training features: {len(training_features)}")
    
    # Create training matrix
    X = df[training_features].copy()
    y = df['total_runs'].copy()
    
    print(f"   Training matrix: {X.shape}")
    print(f"   Target vector: {y.shape}")
    
    return X, y, training_features

def remove_problematic_features(X, y, min_variance=0.001, max_correlation=0.95):
    """
    Remove features that would corrupt training while preserving sophisticated features.
    """
    print(f"\nüßπ FEATURE QUALITY FILTERING")
    print("-" * 50)
    
    initial_features = len(X.columns)
    to_remove = []
    
    # Protect sophisticated features from removal
    sophisticated_protected = [col for col in X.columns if any(keyword in col for keyword in [
        'defensive_efficiency', 'bullpen_fatigue', 'weighted_runs', 
        'xrv_differential', 'clutch_factor', 'offensive_efficiency'
    ])]
    
    print(f"   Protected sophisticated features: {len(sophisticated_protected)}")
    
    # 1. Remove constant features (but protect sophisticated ones)
    for col in X.columns:
        if col not in sophisticated_protected and X[col].var() < min_variance:
            to_remove.append(col)
            print(f"   ‚ùå Constant: {col} (var={X[col].var():.6f})")
    
    # 2. Remove features with too many missing values
    for col in X.columns:
        if col not in to_remove and col not in sophisticated_protected:
            missing_rate = X[col].isnull().mean()
            if missing_rate > 0.5:
                to_remove.append(col)
                print(f"   ‚ùå High missing rate: {col} ({missing_rate*100:.1f}%)")
    
    # 3. Check for extreme correlations (but protect sophisticated features)
    for col in X.columns:
        if col not in to_remove and col not in sophisticated_protected:
            try:
                corr = abs(X[col].corr(y))
                if corr > max_correlation:
                    to_remove.append(col)
                    print(f"   ‚ùå High correlation: {col} (r={corr:.3f})")
            except:
                pass
    
    # Remove problematic features
    if to_remove:
        X_clean = X.drop(columns=to_remove)
        print(f"   Removed: {len(to_remove)} problematic features")
        print(f"   Kept: {initial_features} ‚Üí {len(X_clean.columns)} features")
    else:
        X_clean = X.copy()
        print(f"   ‚úÖ No problematic features found")
    
    # Verify sophisticated features are retained
    retained_sophisticated = [col for col in X_clean.columns if col in sophisticated_protected]
    print(f"   ‚úÖ Retained sophisticated features: {len(retained_sophisticated)}")
    
    return X_clean

def train_comprehensive_model(X, y, test_size=0.2, random_state=42):
    """
    Train comprehensive model with all sophisticated features.
    """
    print(f"\nüöÄ COMPREHENSIVE MODEL TRAINING")
    print("-" * 50)
    print(f"   Features: {X.shape[1]}")
    print(f"   Samples: {X.shape[0]}")
    
    # Handle missing values
    X_filled = X.fillna(X.median())
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X_filled, y, test_size=test_size, random_state=random_state
    )
    
    # Train model with enhanced parameters for sophisticated features
    model = RandomForestRegressor(
        n_estimators=150,  # More trees for sophisticated features
        max_depth=20,      # Deeper trees for complex interactions
        min_samples_split=3,
        min_samples_leaf=1,
        max_features='sqrt',
        random_state=random_state,
        n_jobs=-1
    )
    
    print(f"   Training comprehensive model...")
    model.fit(X_train, y_train)
    
    # Comprehensive evaluation
    train_pred = model.predict(X_train)
    test_pred = model.predict(X_test)
    
    train_mae = mean_absolute_error(y_train, train_pred)
    test_mae = mean_absolute_error(y_test, test_pred)
    train_r2 = r2_score(y_train, train_pred)
    test_r2 = r2_score(y_test, test_pred)
    
    print(f"\nüìä COMPREHENSIVE MODEL PERFORMANCE:")
    print(f"   Training MAE: {train_mae:.3f} runs")
    print(f"   Test MAE: {test_mae:.3f} runs") 
    print(f"   Training R¬≤: {train_r2:.3f}")
    print(f"   Test R¬≤: {test_r2:.3f}")
    print(f"   Training size: {len(X_train)} games")
    print(f"   Test size: {len(X_test)} games")
    
    # Feature importance analysis
    feature_importance = pd.Series(
        model.feature_importances_, 
        index=X.columns
    ).sort_values(ascending=False)
    
    print(f"\nüîç TOP SOPHISTICATED FEATURES:")
    sophisticated_importance = feature_importance[
        feature_importance.index.str.contains('defensive_efficiency|bullpen_fatigue|weighted_runs|xrv_differential|clutch_factor|offensive_efficiency')
    ].head(10)
    
    for i, (feature, importance) in enumerate(sophisticated_importance.items(), 1):
        print(f"   {i:2d}. {feature:<40} {importance:.4f}")
    
    print(f"\nüîç TOP 15 OVERALL FEATURES:")
    for i, (feature, importance) in enumerate(feature_importance.head(15).items(), 1):
        print(f"   {i:2d}. {feature:<40} {importance:.4f}")
    
    return model, feature_importance, train_mae, test_mae, train_r2, test_r2

def main():
    parser = argparse.ArgumentParser(description="Comprehensive 120-day MLB model training")
    parser.add_argument("--comprehensive", action="store_true", 
                       help="Run comprehensive 120-day training with all sophisticated features")
    parser.add_argument("--start", default="2025-03-20", help="Start date")
    parser.add_argument("--end", default="2025-08-21", help="End date")
    parser.add_argument("--deploy", action="store_true", help="Deploy trained model")
    
    args = parser.parse_args()
    
    if not args.comprehensive:
        print("‚ùå Use --comprehensive flag for full sophisticated feature training")
        return 1
    
    print("üöÄ COMPREHENSIVE 120-DAY MODEL TRAINING")
    print("=" * 70)
    print(f"üìÖ Training Period: {args.start} to {args.end}")
    print(f"üéØ Target: 203-feature sophisticated system")
    print(f"üß† Features: All advanced sabermetric calculations")
    print()
    
    # Create models directory
    MODELS_DIR.mkdir(exist_ok=True)
    
    # 1. Fetch comprehensive training data
    df = fetch_comprehensive_training_data(args.start, args.end)
    
    if len(df) < 1000:
        print(f"‚ùå Insufficient data: {len(df)} games (need 1000+)")
        return 1
    
    # 2. Verify sophisticated features
    if not verify_sophisticated_features(df):
        print(f"‚ùå Sophisticated features missing or inadequate")
        return 1
    
    # 3. Prepare comprehensive features
    X, y, feature_names = prepare_comprehensive_features(df)
    
    # 4. Remove problematic features (protecting sophisticated ones)
    X_clean = remove_problematic_features(X, y)
    
    # 5. Train comprehensive model
    model, feature_importance, train_mae, test_mae, train_r2, test_r2 = train_comprehensive_model(X_clean, y)
    
    # 6. Create comprehensive model bundle
    bundle = {
        'model': model,
        'feature_columns': list(X_clean.columns),
        'training_date': datetime.now().isoformat(),
        'training_period': f"{args.start} to {args.end}",
        'training_days': 154,  # 120-day comprehensive window
        'training_size': len(df),
        'train_mae': train_mae,
        'test_mae': test_mae,
        'train_r2': train_r2,
        'test_r2': test_r2,
        'feature_count': len(X_clean.columns),
        'sophisticated_features': len([col for col in X_clean.columns if any(keyword in col for keyword in [
            'defensive_efficiency', 'bullpen_fatigue', 'weighted_runs', 
            'xrv_differential', 'clutch_factor', 'offensive_efficiency'
        ])]),
        'schema_version': 'comprehensive_sophisticated_v1',
        'trainer_version': 'comprehensive_120day_v1',
        'feature_engineering': 'advanced_sabermetric_system',
        'data_source': 'enhanced_games_complete',
        'baseball_intelligence': True,
        'historical_context': True,
        'variance_quality': 'excellent'
    }
    
    # Save comprehensive model
    model_path = MODELS_DIR / "comprehensive_sophisticated_model.joblib"
    joblib.dump(bundle, model_path)
    
    print(f"\nüíæ COMPREHENSIVE MODEL SAVED!")
    print(f"   Path: {model_path}")
    print(f"   Features: {len(X_clean.columns)} (including {bundle['sophisticated_features']} sophisticated)")
    print(f"   Training MAE: {train_mae:.3f} runs")
    print(f"   Test MAE: {test_mae:.3f} runs")
    print(f"   R¬≤ Score: {test_r2:.3f}")
    print(f"   Training Period: 154 days ({len(df)} games)")
    
    if args.deploy:
        # Deploy to production
        production_path = MODELS_DIR / "production_model_latest.joblib"
        joblib.dump(bundle, production_path)
        print(f"üöÄ DEPLOYED TO PRODUCTION: {production_path}")
    
    print(f"\nüéâ COMPREHENSIVE 120-DAY TRAINING COMPLETE!")
    print(f"   Sophisticated advanced features successfully integrated")
    print(f"   Ready for production MLB predictions with baseball intelligence")
    
    return 0

if __name__ == "__main__":
    sys.exit(main())
